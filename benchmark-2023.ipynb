{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ffee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7d450abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T10:55:58.780575Z",
     "iopub.status.busy": "2023-03-31T10:55:58.780166Z",
     "iopub.status.idle": "2023-03-31T10:55:58.992240Z",
     "shell.execute_reply": "2023-03-31T10:55:58.991225Z"
    },
    "papermill": {
     "duration": 0.219589,
     "end_time": "2023-03-31T10:55:58.994949",
     "exception": false,
     "start_time": "2023-03-31T10:55:58.775360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load training dataset, remove missing values in Y, create X and Y matrices.\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "df_train.dropna(subset=['Y'], inplace=True)\n",
    "\n",
    "X = df_train.drop('Y', axis = 1)\n",
    "y = df_train[['Y']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, shuffle=True, random_state=0\n",
    ")\n",
    "\n",
    "#Load test predictors\n",
    "\n",
    "X_test_real = pd.read_csv('data/Xtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7e45e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_grid_params = dict(cv=5, n_jobs=4, verbose=1, scoring='neg_log_loss')\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [100], # 3, 5, 7, 10, 14, 20, 50, 100, 200, 500, 1000\n",
    "    'weights': ['distance'], # 'uniform'\n",
    "    'p': [1], # 2\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l2'],\n",
    "    'C': [100, 1000], # 0.001, 0.01, 0.1, 1, 10, \n",
    "    'fit_intercept': [True], # False\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [5000],\n",
    "    'criterion' : [\"entropy\"],\n",
    "    \"max_depth\" : [400], # 100\n",
    "    'min_samples_split' : [2],\n",
    "    'min_samples_leaf' : [1]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'loss': ['log_loss'],\n",
    "    'learning_rate': [0.02, 0.1, 0.5], # 0.1, 0.5\n",
    "    'n_estimators': [300, 500, 750, 1000], # 100, 200 \n",
    "    'criterion': ['squared_error'],\n",
    "    'max_depth': [None, 2, 5, 10], # None, 2, 10\n",
    "    'validation_fraction' : [0.2],\n",
    "    'n_iter_no_change' : [5],\n",
    "    'tol' : [1e-4]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'loss': ['log_loss'],\n",
    "    'learning_rate': [0.02],\n",
    "    'max_iter': [420],\n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_samples_leaf': [20],\n",
    "    'l2_regularization': [1],\n",
    "    'validation_fraction' : [0.2],\n",
    "    'n_iter_no_change' : [5],\n",
    "    'tol' : [1e-4]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50a12efa",
   "metadata": {},
   "source": [
    "## Pipeline for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a538f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Std(TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        mean_X = np.mean(X, axis=0)\n",
    "        std_X = np.std(X, axis=0)\n",
    "        X = X[(abs(X - mean_X) < 4 * std_X).all(axis=1)]\n",
    "        return X\n",
    "\n",
    "\n",
    "def preprocessing(sub, X_train_sub, X_test_sub):\n",
    "    numeric_features = copy.copy(sub)\n",
    "    categorical_features = list()\n",
    "    if \"X11\" in sub:\n",
    "        numeric_features.remove(\"X11\")\n",
    "        categorical_features.append(\"X11\")\n",
    "    if \"X12\" in sub:\n",
    "        numeric_features.remove(\"X12\")\n",
    "        categorical_features.append(\"X12\")\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')), # mean, median, most_frequent\n",
    "        ('std', Std()),\n",
    "        ('scaler', StandardScaler())], verbose=True)\n",
    "\n",
    "    if len(categorical_features) > 1:\n",
    "        X_train_sub = X_train_sub.astype({'X11':'category', 'X12':'category'})\n",
    "        X_test_sub = X_test_sub.astype({'X11':'category', 'X12':'category'})\n",
    "    elif len(categorical_features) == 1:\n",
    "        if categorical_features[0] == \"X11\":\n",
    "            X_train_sub = X_train_sub.astype({'X11':'category'})\n",
    "            X_test_sub = X_test_sub.astype({'X11':'category'})\n",
    "        else:\n",
    "            X_train_sub = X_train_sub.astype({'X12':'category'})\n",
    "            X_test_sub = X_test_sub.astype({'X12':'category'})\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')), # obligatoire\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))], verbose=True) #Same as pd.get_dummies \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    return preprocessor, X_train_sub, X_test_sub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46d02352",
   "metadata": {},
   "source": [
    "### Permet de vérifier si les colonnes trouvés par la correlation sont bien les mêmes que celles trouvés par combinaison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ee7c32a",
   "metadata": {},
   "source": [
    "### Code générique pour construire notre model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "367d357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit all models \n",
    "sub = [\"X1\", \"X6\", \"X10\", \"X11\", \"X12\"]\n",
    "X_train_sub  = X_train\n",
    "X_test_sub = X_test\n",
    "\n",
    "preprocessor, X_train_sub, X_test_sub = preprocessing(sub, X_train_sub, X_test_sub)\n",
    "\n",
    "grids = {#'KNN': GridSearchCV(KNeighborsClassifier(), param_grid_knn, **default_grid_params),\n",
    "        #'Logistic Regression': GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, **default_grid_params),\n",
    "        'Random Forest': GridSearchCV(ExtraTreesClassifier(), param_grid_rf, **default_grid_params),\n",
    "        #'Gradient Boosting': GridSearchCV(HistGradientBoostingClassifier(), param_grid_hgb, **default_grid_params)\n",
    "        }\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return np.asarray(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f45f54ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 3) Processing imputer, total=   0.0s\n",
      "[Pipeline] ............... (step 2 of 3) Processing std, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 3) Processing scaler, total=   0.0s\n",
      "[Pipeline] ........... (step 1 of 2) Processing imputer, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing onehot, total=   0.0s\n",
      "[Pipeline] ...... (step 1 of 4) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] .......... (step 2 of 4) Processing to_dense, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 4) Processing pca, total=   0.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Pipeline] ........ (step 4 of 4) Processing classifier, total=11.6min\n",
      "Random Forest fitted\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in grids.items():\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('to_dense', DenseTransformer()),\n",
    "                            ('pca', PCA(n_components=0.965, random_state=42, svd_solver='full', n_oversamples=100)),\n",
    "                            ('classifier', model)], verbose=True)\n",
    "    model.fit(X_train_sub, y_train.values.ravel())\n",
    "    grids[model_name] = model\n",
    "    print(f'{model_name} fitted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e748e8ba",
   "metadata": {},
   "source": [
    "## Compute the predicted probability for each class on the training set and evaluate on the log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ca3643bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss on training set...\n",
      "Random Forest: 0.29201919470309745\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-loss on training set...\")\n",
    "log_loss_test = {}\n",
    "for model_name, model in grids.items():\n",
    "    pred_prob_train = pd.DataFrame(model.predict_proba(X_test_sub))\n",
    "    loss = log_loss(y_test, pred_prob_train)\n",
    "    print(f'{model_name}: {loss}')\n",
    "    log_loss_test[model_name] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "44610bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is Random Forest with a log-loss of 0.29201919470309745\n"
     ]
    }
   ],
   "source": [
    "# Best model compare by the log-loss on the test set.\n",
    "best_model = min(log_loss_test, key=log_loss_test.get)\n",
    "print(f'Best model is {best_model} with a log-loss of {log_loss_test[best_model]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56362c2",
   "metadata": {},
   "source": [
    "## Predict on the test predictors, and save the probabilities to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_test = pd.DataFrame(grids[best_model].predict_proba(X_test_real))\n",
    "pred_prob_test.rename(columns = {0: 'Y_1', 1: 'Y_2', 2: 'Y_3', 3: 'Y_4', 4: 'Y_5', 5:'Y_6', 6:'Y_7'}, inplace = True)\n",
    "idx = pred_prob_test.index\n",
    "pred_prob_test.insert(0, 'id', idx)\n",
    "pred_prob_test.to_csv(\"Group13.csv\", index=False)\n",
    "pred_prob_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25b73df",
   "metadata": {},
   "source": [
    "https://askcodez.com/comment-ameliorer-randomforest-la-performance.html\n",
    "\n",
    "https://stats.stackexchange.com/questions/53240/practical-questions-on-tuning-random-forests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.826631,
   "end_time": "2023-03-31T10:56:08.679002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-31T10:55:47.852371",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
